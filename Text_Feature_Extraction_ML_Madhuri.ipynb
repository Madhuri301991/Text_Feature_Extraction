{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34143759",
   "metadata": {},
   "source": [
    "# Text Feature Extraction using ML models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645f4a4",
   "metadata": {},
   "source": [
    "# Installation and Setup\n",
    "\n",
    "### From the command line or terminal:\n",
    "> `conda install scikit-learn`\n",
    "> <br>*or*<br>\n",
    "> `pip install -U scikit-learn`\n",
    "\n",
    "Scikit-learn additionally requires that NumPy and SciPy be installed. For more info visit http://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472ed35",
   "metadata": {},
   "source": [
    "# Perform Imports and Load Data\n",
    "For this exercise we'll be using the **SMSSpamCollection** dataset from [UCI datasets](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) that contains more than 5 thousand SMS phone messages.<br>You can check out the [**sms_readme**](../TextFiles/sms_readme.txt) file for more info.\n",
    "\n",
    "The file is a [tab-separated-values](https://en.wikipedia.org/wiki/Tab-separated_values) (tsv) file with four columns:\n",
    "> **label** - every message is labeled as either ***ham*** or ***spam***<br>\n",
    "> **message** - the message itself<br>\n",
    "> **length** - the number of characters in each message<br>\n",
    "> **punct** - the number of punctuation characters in each message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcee395",
   "metadata": {},
   "source": [
    "# Classification of sms message using length and punct columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ce0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec724951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('smsspamcollection.tsv',sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef03f1",
   "metadata": {},
   "source": [
    "## Check for missing values:\n",
    "Machine learning models usually require complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc16047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()\n",
    "#if anything is missing, it is returning True, nothing is missing, it will return False\n",
    "#False is treated as 0, True is treated as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6083da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "#no data is missing, because we have 0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) #returns the no of rows. we have 5572 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2ae84",
   "metadata": {},
   "source": [
    "## Take a quick look at the *ham* and *spam* `label` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a596ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ada2a",
   "metadata": {},
   "source": [
    "<font color=green>We see that 4825 out of 5572 messages, or 86.6%, are ham.<br>This means that any machine learning model we create has to perform **better than 86.6%** to beat random chance.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc01e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a simple ML model that can predict whether the msg is ham or spam based on length of msg or punct \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16628535",
   "metadata": {},
   "source": [
    "## Visualize the data:\n",
    "Since we're not ready to do anything with the message text, let's see if we can predict ham/spam labels based on message length and punctuation counts. We'll look at message `length` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ffadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72c0b6",
   "metadata": {},
   "source": [
    "<font color=green>This dataset is extremely skewed. The mean value is 80.5 and yet the max length is 910. Let's plot this on a logarithmic x-axis.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam message may be longer than ham message\n",
    "%matplotlib inline\n",
    "\n",
    "plt.xscale('log')\n",
    "bins=1.5**(np.arange(0,15))\n",
    "plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)\n",
    "plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)\n",
    "plt.legend(('ham','spam'))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec51ddb",
   "metadata": {},
   "source": [
    "<font color=green>It looks like there's a small range of values where a message is more likely to be spam than ham.</font>\n",
    "\n",
    "Now let's look at the `punct` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132beff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "bins=1.5**(np.arange(0,15))\n",
    "plt.hist(df[df['label']=='ham']['punct'],bins=bins,alpha=0.8)\n",
    "plt.hist(df[df['label']=='spam']['punct'],bins=bins,alpha=0.8)\n",
    "plt.legend(('ham','spam'))\n",
    "plt.show\n",
    "#no distinct behaviour between ham and spam on the basis of punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98496d",
   "metadata": {},
   "source": [
    "<font color=green>This looks even worse - there seem to be no values where one would pick spam over ham. We'll still try to build a machine learning classification model, but we should expect poor results.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a179c45e",
   "metadata": {},
   "source": [
    "___\n",
    "# Split the data into train & test sets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Feature and label sets\n",
    "X=df[['length','punct']] #note the double set of brackets \n",
    "#passing list of columns so two sets of brackets\n",
    "y=df['label']\n",
    "#X is our feature data\n",
    "#y is our label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c8c8f",
   "metadata": {},
   "source": [
    "## Additional train/test/split arguments:\n",
    "The default test size for `train_test_split` is 30%. Here we'll assign 33% of the data for testing.<br>\n",
    "Also, we can set a `random_state` seed value to ensure that everyone uses the same \"random\" training & testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec88426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de42725",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape #3900 rows with 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21219430",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape #1672 rows with 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245525bd",
   "metadata": {},
   "source": [
    "Now we can pass these sets into a series of different training & testing algorithms and compare their results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5788d16f",
   "metadata": {},
   "source": [
    "___\n",
    "# Train a Logistic Regression classifier\n",
    "One of the simplest multi-class classification tools is [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Scikit-learn offers a variety of algorithmic solvers; we'll use [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0498392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1 import the model;\n",
    "#step2 create an instance of the model\n",
    "#step3 fit the model to the training data\n",
    "#step4 predict the model\n",
    "#solver means what algorithm to use in the optimization problem -for multiclass problems, use lbfgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model=LogisticRegression(solver='lbfgs')\n",
    "lr_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d698f6",
   "metadata": {},
   "source": [
    "## Test the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#Create a predication set:\n",
    "predications=lr_model.predict(X_test)\n",
    "#Print a confusion matrix:\n",
    "print(metrics.confusion_matrix(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72051b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can make the confusion matrix less confusing by adding labels:\n",
    "df_confusion=pd.DataFrame(metrics.confusion_matrix(y_test,predications),index=['ham','spam'],columns=['ham','spam'])\n",
    "df_confusion\n",
    "#correctly classified 1404 as ham  and 5 as spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bba7c",
   "metadata": {},
   "source": [
    "<font color=green>These results are terrible! More spam messages were confused as ham (241) than correctly identified as spam (5), although a relatively small number of ham messages (44) were confused as spam.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predications))\n",
    "#84.3% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c68e5",
   "metadata": {},
   "source": [
    "<font color=green>This model performed *worse* than a classifier that assigned all messages as \"ham\" would have!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fcfdc",
   "metadata": {},
   "source": [
    "___\n",
    "# Train a naïve Bayes classifier:\n",
    "One of the most common - and successful - classifiers is [naïve Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d039b217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_model=MultinomialNB()\n",
    "nb_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019ecc",
   "metadata": {},
   "source": [
    "## Run predictions and report on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23139b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predications=nb_model.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predications))\n",
    "#now we cannot classify any spam messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90623741",
   "metadata": {},
   "source": [
    "<font color=green>The total number of confusions dropped from **287** to **256**. [241+46=287, 246+10=256]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48117e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test,predications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f078eb9",
   "metadata": {},
   "source": [
    "<font color=green>Better, but still less accurate than 86.6%</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddf837",
   "metadata": {},
   "source": [
    "___\n",
    "# Train a support vector machine (SVM) classifier\n",
    "Among the SVM options available, we'll use [C-Support Vector Classification (SVC)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_model=SVC(gamma='auto') #default value\n",
    "svc_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3535a30",
   "metadata": {},
   "source": [
    "## Run predictions and report on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predications=svc_model.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b040719",
   "metadata": {},
   "source": [
    "<font color=green>The total number of confusions dropped even further to **209**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test,predications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test,predications))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b85f51",
   "metadata": {},
   "source": [
    "<font color=green>And finally we have a model that performs *slightly* better than random chance.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
